{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/peppe/01_Study/01_University/Semester/2/Intro_to_ML/Project/Code/models_methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.bypass_bn import enable_running_stats, disable_running_stats\n",
    "from utility.initialize import initialize\n",
    "from utility.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data_loader, optimizer, loss_fn, device, SAM=False, smoothing=0.1, verbose=False, log_interval=10):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # first forward-backward step\n",
    "        if SAM:        \n",
    "            enable_running_stats(model) # disable batch norm running stats\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        if SAM:\n",
    "            loss = loss_fn(outputs, targets, smoothing=smoothing)\n",
    "        else:\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.mean().backward()\n",
    "        \n",
    "        if SAM:\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            # second forward-backward step\n",
    "            disable_running_stats(model)\n",
    "            loss = loss_fn(model(inputs), targets, smoothing=smoothing)\n",
    "            loss.mean().backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.mean().item()\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if verbose and batch_idx % log_interval == 0:\n",
    "            current_loss = cumulative_loss / samples\n",
    "            current_accuracy = cumulative_accuracy / samples * 100\n",
    "            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {current_loss:.4f}, Accuracy: {current_accuracy:.2f}%', end='\\r')\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, data_loader, loss_fn, device):\n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "    cumulative_accuracy = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += loss.mean().item() \n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model,\n",
    "         optimizer,\n",
    "         loss_fn,\n",
    "         data_loaders: dict,\n",
    "         train_step: callable,\n",
    "         test_step: callable,\n",
    "         device,\n",
    "         epochs=10,\n",
    "         exp_name=None,\n",
    "         exp_path=\"/home/peppe/01_Study/01_University/Semester/2/Intro_to_ML/Project/Code/experiments/\",\n",
    "         use_early_stopping=True,\n",
    "         patience=5,\n",
    "         delta=1e-3,\n",
    "         scheduler=None,\n",
    "         verbose_steps=True, # print after log_interval-learning steps\n",
    "         log_interval=10,\n",
    "         use_SAM=False, # if SAM=True then loss_fn must be smooth_cross_entropy with smoothing >= 0.07\n",
    "         smoothing=0.1): \n",
    "    \n",
    "    assert os.path.exists(f\"{exp_path}\"), \"Experiment path does not exist\"\n",
    "    \n",
    "    if use_SAM == True: \n",
    "        assert smoothing >= 0.07, \"smoothing must be >= 0.7 when using SAM\"\n",
    "        assert loss_fn == smooth_crossentropy, \"loss function must be smooth_crossentropy when using SAM\"   \n",
    "        optimizer = SAM(model.parameters(), \n",
    "                        optimizer, \n",
    "                        rho=2, \n",
    "                        adaptive=True, \n",
    "                        lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "            \n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"{exp_path + exp_name}\")\n",
    "\n",
    "    if use_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=patience, \n",
    "                                       delta=delta,\n",
    "                                       path=f\"{exp_path + exp_name + '/checkpoint.pt'}\",)\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    # Computes evaluation results before training\n",
    "    print(\"Before training:\")\n",
    "    train_loss, train_accuracy = test_step(model, data_loaders[\"train_loader\"], loss_fn,device=device)\n",
    "    val_loss, val_accuracy = test_step(model, data_loaders[\"val_loader\"], loss_fn,device=device)\n",
    "    test_loss, test_accuracy = test_step(model, data_loaders[\"test_loader\"], loss_fn,device=device)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    log_values(writer, -1, train_loss, train_accuracy, \"Train\")\n",
    "    log_values(writer, -1, val_loss, val_accuracy, \"Validation\")\n",
    "    log_values(writer, -1, test_loss, test_accuracy, \"Test\")\n",
    "\n",
    "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=\"Training\")\n",
    "    for e in pbar:\n",
    "        train_loss, train_accuracy = train_step(model, data_loaders[\"train_loader\"], optimizer, loss_fn, \n",
    "                                                device=device, SAM=use_SAM, verbose=verbose_steps, log_interval=log_interval)\n",
    "        #if scheduler:\n",
    "        #    scheduler.step()\n",
    "        val_loss, val_accuracy = test_step(model, data_loaders[\"val_loader\"], loss_fn,device=device)\n",
    "        \n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "        # Logs to TensorBoard\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"Train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "        pbar.set_postfix(train_loss=train_loss, train_accuracy=train_accuracy, val_loss=val_loss, val_accuracy=val_accuracy)\n",
    "\n",
    "        if use_early_stopping:\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    # Compute final evaluation results\n",
    "    print(\"After training:\")\n",
    "    train_loss, train_accuracy = test_step(model, data_loaders[\"train_loader\"], loss_fn,device=device)\n",
    "    val_loss, val_accuracy = test_step(model, data_loaders[\"val_loader\"], loss_fn,device=device)\n",
    "    test_loss, test_accuracy = test_step(model, data_loaders[\"test_loader\"], loss_fn,device=device)\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    log_values(writer, epochs + 1, train_loss, train_accuracy, \"Train\")\n",
    "    log_values(writer, epochs + 1, val_loss, val_accuracy, \"Validation\")\n",
    "    log_values(writer, epochs + 1, test_loss, test_accuracy, \"Test\")\n",
    "\n",
    "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # Closes the logger\n",
    "    writer.close()\n",
    "\n",
    "    # Let's return the net\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.SAM.sam import SAM\n",
    "from utility.smooth_cross_entropy import smooth_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transformations to apply to the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "root = \"/home/peppe/01_Study/01_University/Semester/2/Intro_to_ML/Project/data\" #\n",
    "# Load MNIST training dataset\n",
    "trainset = torchvision.datasets.MNIST(root=root, train=True,\n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "\n",
    "# Load MNIST testing dataset\n",
    "testset = torchvision.datasets.MNIST(root=root, train=False,\n",
    "                                     download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the classes in MNIST\n",
    "classes = tuple(str(i) for i in range(10))\n",
    "\n",
    "data_loaders = {\n",
    "    \"train_loader\": trainloader,\n",
    "    \"val_loader\": valloader,\n",
    "    \"test_loader\": testloader\n",
    "}          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simple model \n",
    "import torch.nn.functional as F\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer = SAM(model.parameters(), \n",
    "                base_optimizer, \n",
    "                rho=2, \n",
    "                adaptive=True, \n",
    "                lr=0.1, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "base_optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimizer) == SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "methods.SAM.sam.SAM"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    model,\n",
    "    base_optimizer,\n",
    "    smooth_crossentropy,\n",
    "    train_step=train_step,\n",
    "    test_step=test_step,\n",
    "    data_loaders=data_loaders,\n",
    "    use_SAM=True, \n",
    "    device=device,\n",
    "    exp_name=\"sam_test_4\",\n",
    "    epochs=5,\n",
    "    use_early_stopping=True,\n",
    "    delta=1e-3,\n",
    "    verbose_steps=True,\n",
    "    log_interval=100,\n",
    "    smoothing=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize with tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=/home/peppe/01_Study/01_University/Semester/2/Intro_to_ML/Project/Code/experiments/sam_test_2 # experiment path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
