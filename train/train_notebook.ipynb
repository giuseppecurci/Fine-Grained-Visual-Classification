{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/peppe/01_Study/01_University/Semester/2/Intro_to_ML/Project/Code\") # to import modules from other directories\n",
    "\n",
    "from models_methods.utility.initialize import initialize\n",
    "from models_methods.methods.SAM.sam import SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data_loader, optimizer, loss_fn, device, SAM=False, smoothing=0.1, verbose=False, log_interval=10):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # first forward-backward step\n",
    "        if SAM:        \n",
    "            enable_running_stats(model) # disable batch norm running stats\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        if SAM:\n",
    "            loss = loss_fn(outputs, targets, smoothing=smoothing)\n",
    "        else:\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.mean().backward()\n",
    "        \n",
    "        if SAM:\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            # second forward-backward step\n",
    "            disable_running_stats(model)\n",
    "            loss = loss_fn(model(inputs), targets, smoothing=smoothing)\n",
    "            loss.mean().backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.mean().item()\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if verbose and batch_idx % log_interval == 0:\n",
    "            current_loss = cumulative_loss / samples\n",
    "            current_accuracy = cumulative_accuracy / samples * 100\n",
    "            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {current_loss:.4f}, Accuracy: {current_accuracy:.2f}%', end='\\r')\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, data_loader, loss_fn, device):\n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "    cumulative_accuracy = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += loss.mean().item() \n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from models_methods.methods.SAM.sam import SAM\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = SimpleCNN()\n",
    "optimizer = torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = SAM(model.parameters(), \n",
    "    optimizer, \n",
    "    rho=2, \n",
    "    adaptive=True, \n",
    "    lr=0.1, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model,\n",
    "         optimizer,\n",
    "         loss_fn,\n",
    "         data_loaders: dict,\n",
    "         train_step: callable,\n",
    "         test_step: callable,\n",
    "         device,\n",
    "         epochs=10,\n",
    "         exp_name=None,\n",
    "         exp_path=\"/home/peppe/01_Study/01_University/Semester/2/Intro_to_ML/Project/Code/experiments/\",\n",
    "         use_early_stopping=True,\n",
    "         patience=5,\n",
    "         delta=1e-3,\n",
    "         scheduler=None,\n",
    "         verbose_steps=True, # print after log_interval-learning steps\n",
    "         log_interval=10,\n",
    "         use_SAM=False, # if SAM=True then loss_fn must be smooth_cross_entropy with smoothing >= 0.07\n",
    "         smoothing=0.1): \n",
    "    \n",
    "    assert os.path.exists(f\"{exp_path}\"), \"Experiment path does not exist\"\n",
    "    \n",
    "    if use_SAM == True: \n",
    "        assert smoothing >= 0.07, \"smoothing must be >= 0.7 when using SAM\"\n",
    "        assert loss_fn == smooth_crossentropy, \"loss function must be smooth_crossentropy when using SAM\"   \n",
    "        optimizer = SAM(model.parameters(), \n",
    "                        optimizer, \n",
    "                        rho=2, \n",
    "                        adaptive=True, \n",
    "                        lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "            \n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"{exp_path + exp_name}\")\n",
    "\n",
    "    if use_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=patience, \n",
    "                                       delta=delta,\n",
    "                                       path=f\"{exp_path + exp_name + '/checkpoint.pt'}\",)\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    # Computes evaluation results before training\n",
    "    print(\"Before training:\")\n",
    "    train_loss, train_accuracy = test_step(model, data_loaders[\"train_loader\"], loss_fn,device=device)\n",
    "    val_loss, val_accuracy = test_step(model, data_loaders[\"val_loader\"], loss_fn,device=device)\n",
    "    test_loss, test_accuracy = test_step(model, data_loaders[\"test_loader\"], loss_fn,device=device)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    log_values(writer, -1, train_loss, train_accuracy, \"Train\")\n",
    "    log_values(writer, -1, val_loss, val_accuracy, \"Validation\")\n",
    "    log_values(writer, -1, test_loss, test_accuracy, \"Test\")\n",
    "\n",
    "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=\"Training\")\n",
    "    for e in pbar:\n",
    "        train_loss, train_accuracy = train_step(model, data_loaders[\"train_loader\"], optimizer, loss_fn, \n",
    "                                                device=device, SAM=use_SAM, verbose=verbose_steps, log_interval=log_interval)\n",
    "        #if scheduler:\n",
    "        #    scheduler.step()\n",
    "        val_loss, val_accuracy = test_step(model, data_loaders[\"val_loader\"], loss_fn,device=device)\n",
    "        \n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "        # Logs to TensorBoard\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"Train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "        pbar.set_postfix(train_loss=train_loss, train_accuracy=train_accuracy, val_loss=val_loss, val_accuracy=val_accuracy)\n",
    "\n",
    "        if use_early_stopping:\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    # Compute final evaluation results\n",
    "    print(\"After training:\")\n",
    "    train_loss, train_accuracy = test_step(model, data_loaders[\"train_loader\"], loss_fn,device=device)\n",
    "    val_loss, val_accuracy = test_step(model, data_loaders[\"val_loader\"], loss_fn,device=device)\n",
    "    test_loss, test_accuracy = test_step(model, data_loaders[\"test_loader\"], loss_fn,device=device)\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    log_values(writer, epochs + 1, train_loss, train_accuracy, \"Train\")\n",
    "    log_values(writer, epochs + 1, val_loss, val_accuracy, \"Validation\")\n",
    "    log_values(writer, epochs + 1, test_loss, test_accuracy, \"Test\")\n",
    "\n",
    "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # Closes the logger\n",
    "    writer.close()\n",
    "\n",
    "    # Let's return the net\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize with tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=/home/peppe/01_Study/01_University/Semester/2/Intro_to_ML/Project/Code/experiments/sam_test_2 # experiment path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
